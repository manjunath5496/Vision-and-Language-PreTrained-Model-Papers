<h2>Vision and Language PreTrained Model Papers </h2>



<ul>

                             

 <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(1).pdf" style="text-decoration:none;">Multimodal Machine Learning: A Survey and Taxonomy</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(2).pdf" style="text-decoration:none;">VideoBERT: A Joint Model for Video and Language Representation Learning</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(3).pdf" style="text-decoration:none;">Learning Video Representations using Contrastive Bidirectional Transformer</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(4).pdf" style="text-decoration:none;">Towards Transfer Learning for End-to-End Speech Synthesis from Deep Pre-Trained Language Models</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(5).pdf" style="text-decoration:none;">Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(6).pdf" style="text-decoration:none;">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(7).pdf" style="text-decoration:none;">VisualBERT: A Simple and Performant Baseline for Vision and Language</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(8).pdf" style="text-decoration:none;"> Fusion of Detected Objects in Text for Visual Question Answering </a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(9).pdf" style="text-decoration:none;">Integrating Multimodal Information in Large Pretrained Transformers</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(10).pdf" style="text-decoration:none;">Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(11).pdf" style="text-decoration:none;">LXMERT: Learning Cross-Modality Encoder Representations from Transformers</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(12).pdf" style="text-decoration:none;">VL-BERT: Pre-training of Generic Visual-Linguistic Representations</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(13).pdf" style="text-decoration:none;">Understanding Semantics from Speech Through Pre-training</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(14).pdf" style="text-decoration:none;">Unified Vision-Language Pre-Training for Image Captioning and VQA</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(15).pdf" style="text-decoration:none;">UNITER: UNiversal Image-TExt Representation Learning</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(16).pdf" style="text-decoration:none;">vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations</a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(17).pdf" style="text-decoration:none;">SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering</a></li>   
  
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(18).pdf" style="text-decoration:none;">Effectiveness of self-supervised pre-training for speech recognition</a></li> 

  
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(19).pdf" style="text-decoration:none;">Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA</a></li> 

<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(20).pdf" style="text-decoration:none;">BERT for Large-scale Video Segment Classification with Test-time Augmentation</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(21).pdf" style="text-decoration:none;">12-in-1: Multi-Task Vision and Language Representation Learning</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(22).pdf" style="text-decoration:none;">Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline</a></li> 
 <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(23).pdf" style="text-decoration:none;">Weak Supervision helps Emergence ofWord-Object Alignment and improves Vision-Language Tasks</a></li> 
 

   <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(24).pdf" style="text-decoration:none;">ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(25).pdf" style="text-decoration:none;">Bridging Text and Video: A Universal Multimodal Transformer for Video-Audio Scene-Aware Dialog</a></li>                              
 <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(26).pdf" style="text-decoration:none;">UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(27).pdf" style="text-decoration:none;">Measuring Social Biases in Grounded Vision and Language Embeddings</a></li>
   
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(28).pdf" style="text-decoration:none;">Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(29).pdf" style="text-decoration:none;">What BERT Sees: Cross-Modal Transfer for Visual Question Generation </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(30).pdf" style="text-decoration:none;">XGPT: Cross-modal Generative Pre-Training for Image Captioning</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(31).pdf" style="text-decoration:none;">A Survey on Contextual Embeddings</a></li> 
    <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(32).pdf" style="text-decoration:none;">Pre-trained Models for Natural Language Processing: A Survey</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(33).pdf" style="text-decoration:none;">InterBERT: An Effective Multi-Modal Pretraining Approach via Vision-and-Language Interaction</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(34).pdf" style="text-decoration:none;">Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers</a></li> 
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(35).pdf" style="text-decoration:none;">Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</a></li> 

  <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(36).pdf" style="text-decoration:none;">Are we pretraining it right? Digging deeper into visio-linguistic pretraining</a></li> 
 
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(37).pdf" style="text-decoration:none;">Deep Multimodal Neural Architecture Search</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(38).pdf" style="text-decoration:none;">VD-BERT: A Unified Vision and Dialog Transformer with BERT</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(39).pdf" style="text-decoration:none;">HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(40).pdf" style="text-decoration:none;">MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(41).pdf" style="text-decoration:none;">Cross-Modality Relevance for Reasoning on Language and Vision</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(42).pdf" style="text-decoration:none;">Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(43).pdf" style="text-decoration:none;">Adaptive Transformers for Learning Multimodal Representations</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(44).pdf" style="text-decoration:none;">History for Visual Dialog: Do we really need it?</a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(45).pdf" style="text-decoration:none;">Large-Scale Adversarial Training for Vision-and-Language Representation Learning</a></li>  
   
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(46).pdf" style="text-decoration:none;">Video-Grounded Dialogues with Pretrained Generation Language Models</a></li> 
                             
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(47).pdf" style="text-decoration:none;">ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(48).pdf" style="text-decoration:none;">Auto-captions on GIF: A Large-scale Video-sentence Dataset for Vision-language Pre-training</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(49).pdf" style="text-decoration:none;">DeVLBert: Learning Deconfounded Visio-Linguistic Representations</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(50).pdf" style="text-decoration:none;">CAPT: Contrastive Pre-Training for Learning Denoised Sequence Representations</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(51).pdf" style="text-decoration:none;">Multimodal Pretraining for Dense Video Captioning</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(52).pdf" style="text-decoration:none;">Multimodal Pretraining Unmasked: Unifying the Vision and Language BERTs</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(53).pdf" style="text-decoration:none;">Parameter Efficient Multimodal Transformers for Video Representation Learning</a></li>
 
<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(54).pdf" style="text-decoration:none;">LAMP: Label Augmented Multimodal Pretraining </a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(55).pdf" style="text-decoration:none;">STL-CQA: Structure-based Transformers with Localization and Encoding for Chart Question Answering</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(56).pdf" style="text-decoration:none;">Cross-Probe BERT for Efficient and Effective Cross-Modal Search </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(57).pdf" style="text-decoration:none;">SemVLP: Vision-Language Pre-training by Aligning Semantics at Multiple Levels</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(58).pdf" style="text-decoration:none;">Multi-Modality Cross Attention Network for Image and Sentence Matching</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(59).pdf" style="text-decoration:none;">ActBERT: Learning Global-Local Video-Text Representations</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(60).pdf" style="text-decoration:none;">A Comprehensive Survey of Deep Learning for Image Captioning </a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Vision-and-Language-PreTrained-Model-Papers/blob/master/vi(61).pdf" style="text-decoration:none;">Deep Multimodal Representation Learning: A Survey</a></li>
 
   </ul>
